{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad292bc7-17d5-4e60-af5c-6508b097244f",
   "metadata": {},
   "source": [
    "# Learn How to Move CDC Enabled ScyllaDB Tables into Hudi Datalakes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c9b4b-22a5-42f1-9810-77b22d236c81",
   "metadata": {},
   "source": [
    "# prerequisites \n",
    "\n",
    "### Docker compose file \n",
    "```\n",
    "version: '3'\n",
    "\n",
    "services:\n",
    "  scylla:\n",
    "    image: scylladb/scylla:4.6.0\n",
    "    expose:\n",
    "      - \"9042\"\n",
    "    ports:\n",
    "      - \"9042:9042\"\n",
    "```\n",
    "\n",
    "\n",
    "#### Create Samople table\n",
    "```\n",
    "-- Create Keyspace if not already created\n",
    "CREATE KEYSPACE IF NOT EXISTS my_keyspace\n",
    "WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\n",
    "USE my_keyspace;\n",
    "\n",
    "-- Create Table with new columns: city, state, and ts (auto-generated timestamp)\n",
    "-- Create Table with CDC enabled\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    user_id int PRIMARY KEY,\n",
    "    name text,\n",
    "    age int,\n",
    "    city text,\n",
    "    state text\n",
    ") WITH cdc = {'enabled': true};  -- Enabling CDC\n",
    "\n",
    "-- Insert Data (you can leave out the ts column as it will be auto-generated)\n",
    "INSERT INTO users (user_id, name, age, city, state)\n",
    "VALUES (1, 'Alice', 30, 'Seattle', 'WA');\n",
    "\n",
    "INSERT INTO users (user_id, name, age, city, state)\n",
    "VALUES (2, 'Bob', 25, 'San Francisco', 'CA');\n",
    "\n",
    "INSERT INTO users (user_id, name, age, city, state)\n",
    "VALUES (3, 'Charlie', 28, 'Austin', 'TX');\n",
    "\n",
    "-- Select to verify inserted data\n",
    "SELECT * FROM users;\n",
    "```\n",
    "\n",
    "### Install DEP\n",
    "```\n",
    "!/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install pyspark==3.4.0\n",
    "!/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install cassandra-driver\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5048c23d-3f50-4ffb-be70-ed4966b1d63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mcheckpoints\u001b[m\u001b[m         \u001b[31mdocker-compose.yaml\u001b[m\u001b[m \u001b[31mlab1.py\u001b[m\u001b[m             \u001b[31mscyla_batch.py\u001b[m\u001b[m\n",
      "\u001b[31mdemo.sql\u001b[m\u001b[m            \u001b[31mhudi-scylladb.ipynb\u001b[m\u001b[m \u001b[31mnotes\u001b[m\u001b[m               \u001b[31mspark_lab1.py\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cb2258-d04e-4af2-9724-6b65c04e0c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/0)                                    docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 2/0\n",
      " \u001b[32m笨能u001b[0m Network sylladb_default     \u001b[32mCreated\u001b[0m                                     \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m笨能u001b[0m Container sylladb-scylla-1  \u001b[32mCreated\u001b[0m                                     \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 2/2\u001b[0m\n",
      " \u001b[32m笨能u001b[0m Network sylladb_default     \u001b[32mCreated\u001b[0m                                     \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m笨能u001b[0m Container sylladb-scylla-1  \u001b[32mStarted\u001b[0m                                     \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! docker-compose up --build -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85007d79-6573-4c4d-96f5-5325ef449e40",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2aba06e-4ccc-43f9-8665-235b11c30300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: className\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.prefetch.enable\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.experimental.fadvise\n",
      "Ivy Default Cache set to: /Users/soumilshah/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/soumilshah/.ivy2/jars\n",
      "org.apache.hudi#hudi-spark3.4-bundle_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cd51692b-95db-42e4-8485-efbdf61670a0;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hudi#hudi-spark3.4-bundle_2.12;0.14.0 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in local-m2-cache\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.773 in central\n",
      "downloading file:/Users/soumilshah/.m2/repository/org/apache/hudi/hudi-spark3.4-bundle_2.12/0.14.0/hudi-spark3.4-bundle_2.12-0.14.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hudi#hudi-spark3.4-bundle_2.12;0.14.0!hudi-spark3.4-bundle_2.12.jar (137ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (127ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.773/aws-java-sdk-bundle-1.12.773.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.773!aws-java-sdk-bundle.jar (6407ms)\n",
      "downloading file:/Users/soumilshah/.m2/repository/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (5ms)\n",
      ":: resolution report :: resolve 3176ms :: artifacts dl 6682ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.773 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hudi#hudi-spark3.4-bundle_2.12;0.14.0 from local-m2-cache in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from local-m2-cache in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.773] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   4   |   4   |   1   ||   4   |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cd51692b-95db-42e4-8485-efbdf61670a0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/8ms)\n",
      "24/10/18 18:58:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os, sys\n",
    "\n",
    "# Set Java Home environment variable if needed\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@11\"\n",
    "\n",
    "HUDI_VERSION = '0.14.0'\n",
    "SPARK_VERSION = '3.4'\n",
    "\n",
    "SUBMIT_ARGS = f\"--packages org.apache.hudi:hudi-spark{SPARK_VERSION}-bundle_2.12:{HUDI_VERSION},org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.773 pyspark-shell\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config('spark.sql.extensions', 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension') \\\n",
    "    .config('className', 'org.apache.hudi') \\\n",
    "    .config(\"fs.s3a.prefetch.enable\", \"false\") \\\n",
    "    .config(\"fs.s3a.experimental.fadvise\", \"random\") \\\n",
    "    .config('spark.sql.hive.convertMetastoreParquet', 'false') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb78f55-ada3-4914-aaa7-4cc6324dec88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://soumils-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10b5af8d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d7461-b179-4a2d-91e1-a2f067c9ad5b",
   "metadata": {},
   "source": [
    "# Python helper class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7071adc1-2f4a-4265-80b8-f2c27d7c6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import json, os\n",
    "    from cassandra.cluster import Cluster\n",
    "    from cassandra.query import dict_factory\n",
    "    from datetime import datetime\n",
    "    from datetime import datetime, timezone\n",
    "    from uuid import UUID\n",
    "except Exception as e:\n",
    "    print(\"Library Missing \", e)\n",
    "\n",
    "\n",
    "class ScyllaCDCReader:\n",
    "    def __init__(self, host='127.0.0.1', keyspace='my_keyspace', table_name='users_scylla_cdc_log',\n",
    "                 checkpoint_dir='checkpoints', batch_size=10):\n",
    "        self.cluster = Cluster([host])\n",
    "        self.session = self.cluster.connect(keyspace)\n",
    "        self.session.row_factory = dict_factory  # This will return rows as dictionaries\n",
    "        self.table_name = table_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, f'{table_name}_checkpoint.json')\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def read_checkpoint(self):\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                checkpoint = json.load(f)\n",
    "            return UUID(checkpoint['last_cdc_time'])\n",
    "        return UUID('00000000-0000-1000-8080-808080808080')\n",
    "\n",
    "    def commit_checkpoint(self, last_cdc_time):\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump({'last_cdc_time': str(last_cdc_time)}, f)\n",
    "        print(f\"Checkpoint committed: {last_cdc_time}\")\n",
    "\n",
    "    def get_cdc_logs(self, start_time=None):\n",
    "        if start_time is None:\n",
    "            query = f'SELECT * FROM {self.table_name}'\n",
    "        else:\n",
    "            query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {self.table_name}\n",
    "            WHERE \"cdc$time\" > {start_time}\n",
    "            ALLOW FILTERING\n",
    "            \"\"\"\n",
    "        return self.session.execute(query)\n",
    "\n",
    "    def get_messages(self):\n",
    "        start_time = self.read_checkpoint()\n",
    "        rows = self.get_cdc_logs(start_time)\n",
    "\n",
    "        batch = []\n",
    "        for row in rows:\n",
    "            batch.append(row)\n",
    "            if len(batch) >= self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        # Yield any remaining messages in the last batch\n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "    def close(self):\n",
    "        self.session.shutdown()\n",
    "        self.cluster.shutdown()\n",
    "\n",
    "\n",
    "def create_cdc_reader(host, keyspace, table, checkpoint_dir, batch_size=10):\n",
    "    return ScyllaCDCReader(host=host, keyspace=keyspace, table_name=table,\n",
    "                           checkpoint_dir=checkpoint_dir, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def sanitize_column_names(row):\n",
    "    return {k.replace('$', '_'): v for k, v in row.items()}\n",
    "\n",
    "\n",
    "def uuid_to_datetime(uuid_obj):\n",
    "    timestamp = uuid_obj.time / 1e7  # Convert 100-nanosecond intervals to seconds\n",
    "    return datetime.fromtimestamp(timestamp, tz=timezone.utc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165d731-f437-449a-b027-514a0d99313f",
   "metadata": {},
   "source": [
    "# Method to Write into Hudi table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90717c57-cee0-4e72-bae0-070d96beeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_hudi(spark_df, \n",
    "                  table_name, \n",
    "                  db_name, \n",
    "                  method='upsert',\n",
    "                  table_type='COPY_ON_WRITE',\n",
    "                  recordkey='',\n",
    "                  precombine='',\n",
    "                  partition_fields='',\n",
    "                  index_type='RECORD_INDEX'\n",
    "                 ):\n",
    "\n",
    "    path = f\"file:///Users/soumilshah/Desktop/hudi/database={db_name}/table_name={table_name}\"\n",
    "\n",
    "    hudi_options = {\n",
    "        'hoodie.table.name': table_name,\n",
    "        'hoodie.datasource.write.table.type': table_type,\n",
    "        'hoodie.datasource.write.table.name': table_name,\n",
    "        'hoodie.datasource.write.operation': method,\n",
    "        'hoodie.datasource.write.recordkey.field': recordkey,\n",
    "        'hoodie.datasource.write.precombine.field': precombine,\n",
    "        \"hoodie.datasource.write.partitionpath.field\": partition_fields, \n",
    "         'hoodie.datasource.write.payload.class': 'org.apache.hudi.common.model.PartialUpdateAvroPayload'\n",
    "        \n",
    "    }\n",
    "\n",
    "    \n",
    "    spark_df.write.format(\"hudi\"). \\\n",
    "        options(**hudi_options). \\\n",
    "        mode(\"append\"). \\\n",
    "        save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1a9ec-7322-4445-9ac0-aebd4665076e",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "524b2640-4d95-453d-af74-4c52367601ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint committed: 2cc3ea74-8da5-11ef-98e5-eef8b009db9e\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
    "\n",
    "\n",
    "reader = create_cdc_reader('127.0.0.1',\n",
    "                           'my_keyspace',\n",
    "                           'users_scylla_cdc_log',\n",
    "                           './checkpoints',\n",
    "                           batch_size=1000)\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"cdc_time\", StringType(), True),\n",
    "    StructField(\"cdc_batch_seq_no\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"cdc_deleted_age\", IntegerType(), True),\n",
    "    StructField(\"cdc_deleted_city\", StringType(), True),\n",
    "    StructField(\"cdc_deleted_name\", StringType(), True),\n",
    "    StructField(\"cdc_deleted_state\", StringType(), True),\n",
    "    StructField(\"cdc_end_of_batch\", BooleanType(), True),\n",
    "    StructField(\"cdc_operation\", IntegerType(), True),\n",
    "    StructField(\"cdc_ttl\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"readable_timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "try:\n",
    "    for batch in reader.get_messages():\n",
    "        sanitized_batch = [sanitize_column_names(row) for row in batch]\n",
    "        for row in sanitized_batch:\n",
    "            row.pop(\"cdc_stream_id\")\n",
    "            readable_timestamp = uuid_to_datetime((row['cdc_time']))\n",
    "            row['cdc_time'] = str(row['cdc_time'])\n",
    "            row['readable_timestamp'] = readable_timestamp.isoformat()\n",
    "\n",
    "        \"\"\"Create DF \"\"\"\n",
    "        df = spark.createDataFrame(sanitized_batch, schema=schema)        \n",
    "        write_to_hudi(\n",
    "            spark_df=df,\n",
    "            db_name=\"default\",\n",
    "            table_name=\"users_scylla_cdc_log\",\n",
    "            recordkey=\"user_id\",\n",
    "            precombine=\"readable_timestamp\",\n",
    "            partition_fields=\"\"\n",
    "        )\n",
    "\n",
    "        if sanitized_batch:\n",
    "            pass\n",
    "            reader.commit_checkpoint(sanitized_batch[-1]['cdc_time'])\n",
    "finally:\n",
    "    reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1aeb50-3307-4492-8c35-a3006a6402cb",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f723dbe-1bb5-4ad4-ad17-591ab2efb9d8",
   "metadata": {},
   "source": [
    "# Read from hudi table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2abb3891-df66-44cd-9eef-50827ca0a5e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------+---+\n",
      "|user_id|state|         city|age|\n",
      "+-------+-----+-------------+---+\n",
      "|      2|   CA|San Francisco| 25|\n",
      "|      1|   WA|      Seattle| 30|\n",
      "|      3|   TX|       Austin| 28|\n",
      "+-------+-----+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"file:///Users/soumilshah/Desktop/hudi/database=default/table_name=users_scylla_cdc_log\"\n",
    "\n",
    "spark.read.format(\"hudi\") \\\n",
    "    .load(path) \\\n",
    "    .createOrReplaceTempView(\"snapshot\")\n",
    "\n",
    "spark.sql(\"select user_id,state,city,age from snapshot\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
